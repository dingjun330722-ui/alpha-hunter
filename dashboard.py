import streamlit as st
import requests
import feedparser
from openai import OpenAI
import datetime
import concurrent.futures
import json
import os
import socket

# ================== é¡µé¢é…ç½® ==================
st.set_page_config(page_title="Alpha Hunter V2.4 (ç©¿å¢™ç‰ˆ)", page_icon="âš¡", layout="wide")

# ================== å…¨å±€è®¾ç½® ==================
# 1. æ”¾å®½è¶…æ—¶æ—¶é—´åˆ° 30ç§’ (é˜²æ­¢ç½‘ç»œæ³¢åŠ¨)
socket.setdefaulttimeout(30)

# 2. ä¼ªè£…æµè§ˆå™¨å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# è‡ªå®šä¹‰ CSS
st.markdown("""
<style>
    .card { background-color: #f0f2f6; border-radius: 10px; padding: 20px; border-left: 5px solid #ff4b4b; margin-bottom: 20px; color: #31333F; }
    .card-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
    .card-content { font-size: 14px; line-height: 1.6; }
    .card-source { font-size: 12px; color: #666; margin-top: 15px; font-style: italic; }
    .stButton>button { width: 100%; }
</style>
""", unsafe_allow_html=True)

# ================== é…ç½®ç®¡ç† ==================
SOURCE_FILE = "sources.json"
CONFIG_FILE = "config.json"

DEFAULT_CONFIG = {
    "api_url": "https://new.wuxuai.com/v1", # ä½ çš„API
    "api_key": "",
    "proxy_url": "", # æ–°å¢ï¼šä»£ç†åœ°å€
    "models": ["gemini-2.5-pro", "gpt-4o", "glm-4-flash"],
    "selected_model": "gemini-2.5-pro"
}

def load_config():
    if not os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(DEFAULT_CONFIG, f, ensure_ascii=False, indent=4)
        return DEFAULT_CONFIG
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return DEFAULT_CONFIG

def save_config(config_data):
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config_data, f, ensure_ascii=False, indent=4)

if 'app_config' not in st.session_state:
    st.session_state.app_config = load_config()

def update_config_key(key, value):
    st.session_state.app_config[key] = value
    save_config(st.session_state.app_config)

# ================== AI åˆ†ææ ¸å¿ƒ ==================
def analyze_single_source(source, model, key, url, sys_prompt, proxy):
    result = {"source": source["name"], "status": "failed", "data": None, "error": None}
    
    # === å…³é”®ä¿®æ­£ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨ä»£ç† ===
    if proxy and proxy.strip() != "":
        os.environ['http_proxy'] = proxy
        os.environ['https_proxy'] = proxy
    else:
        # å¦‚æœæ²¡å¡«ä»£ç†ï¼Œæ¸…é™¤ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢æ®‹ç•™
        os.environ.pop('http_proxy', None)
        os.environ.pop('https_proxy', None)

    if not source.get("enabled", True):
        result["status"] = "skipped"
        return result

    try:
        # è§£æ RSS
        feed = feedparser.parse(source["url"], request_headers=HEADERS)
        
        if not feed.entries:
            result["status"] = "empty"
            return result
            
        entry = feed.entries[0]
        content_snippet = entry.get('summary', '')[:800]
        if len(content_snippet) < 10: content_snippet = entry.title # ä¿åº•

        # è°ƒç”¨ AI (OpenAI åº“ä¼šè‡ªåŠ¨è¯»å–ä¸Šé¢çš„ç¯å¢ƒå˜é‡ä»£ç†ï¼Œæˆ–è€…ç›´è¿)
        client = OpenAI(api_key=key, base_url=url)
        
        user_prompt = f"ã€æ ‡é¢˜ã€‘ï¼š{entry.title}\nã€å†…å®¹æ‘˜è¦ã€‘ï¼š{content_snippet}"
        
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_prompt}
            ],
            timeout=20 
        )
        
        result["status"] = "success"
        result["data"] = {
            "title": entry.title,
            "link": entry.link,
            "summary": entry.get('summary', 'æ— æ‘˜è¦'),
            "ai_analysis": resp.choices[0].message.content,
        }
        return result
    except Exception as e:
        result["error"] = str(e)
        return result

# ================== ä¾§è¾¹æ  ==================
with st.sidebar:
    st.header("âš™ï¸ ç©¿å¢™æ§åˆ¶å°")
    
    with st.expander("ğŸ”Œ è¿æ¥é…ç½®", expanded=True):
        api_url = st.text_input("æ¥å£åœ°å€", value=st.session_state.app_config.get("api_url"), key="input_url", on_change=lambda: update_config_key("api_url", st.session_state.input_url))
        api_key = st.text_input("API å¯†é’¥", type="password", value=st.session_state.app_config.get("api_key"), key="input_key", on_change=lambda: update_config_key("api_key", st.session_state.input_key))
        
        # === æ–°å¢ï¼šä»£ç†è®¾ç½® ===
        st.markdown("---")
        st.caption("ğŸ‘‡ å¦‚æœå…¨è·³è¿‡ï¼Œè¯·åœ¨æ­¤å¡«å…¥æœ¬åœ°ä»£ç†åœ°å€ (å¦‚ http://127.0.0.1:7890)")
        proxy_url = st.text_input("HTTP ä»£ç† (Proxy)", value=st.session_state.app_config.get("proxy_url", ""), placeholder="ä¾‹å¦‚: http://127.0.0.1:7890", key="input_proxy", on_change=lambda: update_config_key("proxy_url", st.session_state.input_proxy))

    st.markdown("### ğŸ¤– æ¨¡å‹æ§åˆ¶")
    # ... (æ¨¡å‹é€‰æ‹©éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œçœç•¥ä»¥èŠ‚çœç©ºé—´ï¼ŒåŠŸèƒ½åŒä¸Šç‰ˆæœ¬) ...
    # ä¸ºäº†ä¿è¯å®Œæ•´æ€§ï¼Œæˆ‘è¿™é‡Œä¿ç•™æ ¸å¿ƒä¸‹æ‹‰æ¡†é€»è¾‘
    model_list = st.session_state.app_config.get("models", ["gemini-2.5-pro"])
    current_model = st.session_state.app_config.get("selected_model")
    index = model_list.index(current_model) if current_model in model_list else 0
    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_list, index=index, key="model_select", on_change=lambda: update_config_key("selected_model", st.session_state.model_select))
    
    # åˆ·æ–°æŒ‰é’®é€»è¾‘ç®€å•åŒ–
    if st.button("ğŸ”„ åˆ·æ–°æ¨¡å‹åº“"):
         try:
            # ç®€å•åˆ·æ–°é€»è¾‘
            os.environ['http_proxy'] = proxy_url
            os.environ['https_proxy'] = proxy_url
            headers = {"Authorization": f"Bearer {api_key}"}
            res = requests.get(f"{api_url.rstrip('/')}/models", headers=headers, timeout=10)
            if res.status_code == 200:
                data = res.json()
                models = [item['id'] for item in data['data']] if 'data' in data else [item['id'] for item in data]
                st.session_state.app_config["models"] = models
                save_config(st.session_state.app_config)
                st.success("åˆ·æ–°æˆåŠŸ")
                st.rerun()
         except: st.error("åˆ·æ–°å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–å¯†é’¥")

    st.divider()
    st.markdown("### ğŸ“¡ æƒ…æŠ¥æºç®¡ç†")
    if 'sources_data' not in st.session_state:
        if os.path.exists(SOURCE_FILE):
             with open(SOURCE_FILE, 'r', encoding='utf-8') as f: st.session_state.sources_data = json.load(f)
        else: st.session_state.sources_data = [{"name": "OpenAI Blog", "url": "https://openai.com/index.xml", "enabled": True}]

    edited_sources = st.data_editor(st.session_state.sources_data, num_rows="dynamic", column_config={"name": "ä¿¡æºåç§°","url": st.column_config.LinkColumn("RSSé“¾æ¥"),"enabled": st.column_config.CheckboxColumn("å¯ç”¨", default=True)}, key="editor")
    if edited_sources != st.session_state.sources_data:
        st.session_state.sources_data = edited_sources
        with open(SOURCE_FILE, 'w', encoding='utf-8') as f: json.dump(edited_sources, f, ensure_ascii=False, indent=4)

    st.divider()
    default_prompt = "ä½ æ˜¯ä¸€ä¸ªåå°”è¡—é¡¶çº§æƒ…æŠ¥å®˜ã€‚å¯¹æ¯æ¡æ¶ˆæ¯è¿›è¡Œè¯„åˆ†(0-10)ã€‚ç»™å‡ºç®€ç»ƒçš„ã€é€»è¾‘é“¾ã€‘æ¨æ¼”å’Œã€äº¤æ˜“å»ºè®®ã€‘(Long/Short)ã€‚é£æ ¼æåº¦æ¯’èˆŒã€åŠŸåˆ©ã€‚"
    system_prompt = st.text_area("AI äººè®¾æŒ‡ä»¤", value=default_prompt, height=100)

# ================== ä¸»ç•Œé¢ ==================
st.title("âš¡ Alpha Hunter V2.4 (ç©¿å¢™ç‰ˆ)")

if st.button("ğŸš€ æé€Ÿæ‰«æ (TURBO SCAN)", type="primary"):
    active_sources = [s for s in st.session_state.sources_data if s.get('enabled', True)]
    
    if not active_sources:
        st.warning("è¯·å…ˆæ·»åŠ æƒ…æŠ¥æºï¼")
    else:
        results_container = st.container()
        progress_bar = st.progress(0)
        
        # ä¼ å…¥ proxy å‚æ•°
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_source = {
                executor.submit(analyze_single_source, source, selected_model, api_key, api_url, system_prompt, proxy_url): source 
                for source in active_sources
            }
            
            completed = 0
            for future in concurrent.futures.as_completed(future_to_source):
                res = future.result()
                completed += 1
                progress_bar.progress(completed / len(active_sources))
                
                if res["status"] == "success":
                    data = res["data"]
                    with results_container:
                        c1, c2 = st.columns([1.5, 1])
                        with c2:
                            st.subheader(f"ğŸ“„ {res['source']}")
                            st.markdown(f"[{data['title']}]({data['link']})")
                            with st.expander("æ‘˜è¦"): st.write(data['summary'])
                            html_card = f"""<div class="card"><div class="card-title">{data['title']}</div><div class="card-content">{data['summary'][:150]}...</div><div class="card-source">Source: {res['source']}</div></div>"""
                            st.markdown(html_card, unsafe_allow_html=True)
                        with c1:
                            st.subheader("ğŸ¤– åˆ†ææŠ¥å‘Š")
                            st.info(data['ai_analysis'])
                        st.divider()
                elif res["status"] == "failed":
                    error_msg = res['error']
                    # ä¼˜åŒ–æŠ¥é”™æ˜¾ç¤º
                    if "Connection" in str(error_msg) or "timed out" in str(error_msg):
                        st.warning(f"âš ï¸ {res['source']} æ— æ³•è¿æ¥ (è¯·æ£€æŸ¥ä»£ç†è®¾ç½®)")
                    else:
                        st.error(f"âŒ {res['source']}: {error_msg}")